# Training configuration for EfficientNet-b0 trained on ImageNet on TPUs.
# Takes ~2 minutes, 50 seconds per epoch for v3-32.
# Reaches ~76.1% within 350 epochs.
# Note: This configuration uses a scaled per-replica batch size based on the number of devices.
runtime:
  distribution_strategy: 'tpu'
train_dataset:
  name: 'cifar10'
  data_dir: null
  builder: 'records'
  split: 'train'
  one_hot: false
  image_size: 32
  num_classes: 10
  num_examples: 50000
  batch_size: 128
  use_per_replica_batch_size: true
  mean_subtract: false
  standardize: false
  dtype: 'bfloat16'
  filenames: ['cifar10-train.tfrecord-00000-of-00001']
validation_dataset:
  name: 'cifar10'
  data_dir: null
  builder: 'records'
  split: 'validation'
  one_hot: false
  image_size: 32
  num_classes: 10
  num_examples: 10000
  batch_size: 128
  use_per_replica_batch_size: true
  mean_subtract: false
  standardize: false
  dtype: 'bfloat16'
  filenames: ['cifar10-test.tfrecord-00000-of-00001']
model:
  model_params:
    model_name: 'efficientnet-b0'
    overrides:
      num_classes: 10
      batch_norm: 'tpu'
      dtype: 'bfloat16'
      activation: 'swish'
  optimizer:
    name: 'rmsprop'
    momentum: 0.9
    decay: 0.9
    moving_average_decay: 0.0
    lookahead: false
  learning_rate:
    name: 'exponential'
  loss:
    label_smoothing: 0.1
train:
  resume_checkpoint: true
  epochs: 10
  set_epoch_loop: true
evaluation:
  epochs_between_evals: 1
